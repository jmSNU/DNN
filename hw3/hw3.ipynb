{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem1 : Implementation\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongminlee/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 6749.13623046875\n",
      "epoch : 1, train_loss : 12.138482093811035\n",
      "epoch : 2, train_loss : 2.8328053951263428\n",
      "epoch : 3, train_loss : 2.8810248374938965\n",
      "epoch : 4, train_loss : 2.882246732711792\n",
      "epoch : 5, train_loss : 2.8820912837982178\n",
      "epoch : 6, train_loss : 2.8819119930267334\n",
      "epoch : 7, train_loss : 2.881739616394043\n",
      "epoch : 8, train_loss : 2.8815760612487793\n",
      "epoch : 9, train_loss : 2.8814194202423096\n",
      "epoch : 10, train_loss : 2.881269693374634\n",
      "epoch : 11, train_loss : 2.881124496459961\n",
      "epoch : 12, train_loss : 2.880984306335449\n",
      "epoch : 13, train_loss : 2.8808491230010986\n",
      "epoch : 14, train_loss : 2.8807179927825928\n",
      "epoch : 15, train_loss : 2.8805902004241943\n",
      "epoch : 16, train_loss : 2.8804657459259033\n",
      "epoch : 17, train_loss : 2.880344867706299\n",
      "epoch : 18, train_loss : 2.8802270889282227\n",
      "epoch : 19, train_loss : 2.8801095485687256\n",
      "epoch : 20, train_loss : 2.8799962997436523\n",
      "epoch : 21, train_loss : 2.8798837661743164\n",
      "epoch : 22, train_loss : 2.879772901535034\n",
      "epoch : 23, train_loss : 2.8796629905700684\n",
      "epoch : 24, train_loss : 2.879556179046631\n",
      "epoch : 25, train_loss : 2.8794498443603516\n",
      "epoch : 26, train_loss : 2.8793439865112305\n",
      "epoch : 27, train_loss : 2.879239320755005\n",
      "epoch : 28, train_loss : 2.8791348934173584\n",
      "epoch : 29, train_loss : 2.8790314197540283\n",
      "epoch : 30, train_loss : 2.8789286613464355\n",
      "epoch : 31, train_loss : 2.8788270950317383\n",
      "epoch : 32, train_loss : 2.87872314453125\n",
      "epoch : 33, train_loss : 2.8786213397979736\n",
      "epoch : 34, train_loss : 2.878519296646118\n",
      "epoch : 35, train_loss : 2.8784170150756836\n",
      "epoch : 36, train_loss : 2.8783156871795654\n",
      "epoch : 37, train_loss : 2.8782126903533936\n",
      "epoch : 38, train_loss : 2.87811017036438\n",
      "epoch : 39, train_loss : 2.8780086040496826\n",
      "epoch : 40, train_loss : 2.8779051303863525\n",
      "epoch : 41, train_loss : 2.8778014183044434\n",
      "epoch : 42, train_loss : 2.8776984214782715\n",
      "epoch : 43, train_loss : 2.8775928020477295\n",
      "epoch : 44, train_loss : 2.877487897872925\n",
      "epoch : 45, train_loss : 2.877382278442383\n",
      "epoch : 46, train_loss : 2.8772759437561035\n",
      "epoch : 47, train_loss : 2.8771700859069824\n",
      "epoch : 48, train_loss : 2.877061367034912\n",
      "epoch : 49, train_loss : 2.876953125\n",
      "epoch : 50, train_loss : 2.876842737197876\n",
      "epoch : 51, train_loss : 2.8767342567443848\n",
      "epoch : 52, train_loss : 2.876621723175049\n",
      "epoch : 53, train_loss : 2.876509189605713\n",
      "epoch : 54, train_loss : 2.8763976097106934\n",
      "epoch : 55, train_loss : 2.876284122467041\n",
      "epoch : 56, train_loss : 2.876168966293335\n",
      "epoch : 57, train_loss : 2.8760533332824707\n",
      "epoch : 58, train_loss : 2.8759379386901855\n",
      "epoch : 59, train_loss : 2.875819206237793\n",
      "epoch : 60, train_loss : 2.875702381134033\n",
      "epoch : 61, train_loss : 2.875581979751587\n",
      "epoch : 62, train_loss : 2.8754615783691406\n",
      "epoch : 63, train_loss : 2.8753416538238525\n",
      "epoch : 64, train_loss : 2.8752174377441406\n",
      "epoch : 65, train_loss : 2.8750975131988525\n",
      "epoch : 66, train_loss : 2.8749728202819824\n",
      "epoch : 67, train_loss : 2.8748483657836914\n",
      "epoch : 68, train_loss : 2.874723196029663\n",
      "epoch : 69, train_loss : 2.8745973110198975\n",
      "epoch : 70, train_loss : 2.87446928024292\n",
      "epoch : 71, train_loss : 2.874343156814575\n",
      "epoch : 72, train_loss : 2.8742153644561768\n",
      "epoch : 73, train_loss : 2.874086380004883\n",
      "epoch : 74, train_loss : 2.8739569187164307\n",
      "epoch : 75, train_loss : 2.8738255500793457\n",
      "epoch : 76, train_loss : 2.8736956119537354\n",
      "epoch : 77, train_loss : 2.873563766479492\n",
      "epoch : 78, train_loss : 2.8734312057495117\n",
      "epoch : 79, train_loss : 2.873298168182373\n",
      "epoch : 80, train_loss : 2.8731658458709717\n",
      "epoch : 81, train_loss : 2.873030185699463\n",
      "epoch : 82, train_loss : 2.872894763946533\n",
      "epoch : 83, train_loss : 2.8727574348449707\n",
      "epoch : 84, train_loss : 2.872620105743408\n",
      "epoch : 85, train_loss : 2.8724822998046875\n",
      "epoch : 86, train_loss : 2.8723392486572266\n",
      "epoch : 87, train_loss : 2.8721983432769775\n",
      "epoch : 88, train_loss : 2.872053861618042\n",
      "epoch : 89, train_loss : 2.8719074726104736\n",
      "epoch : 90, train_loss : 2.8717572689056396\n",
      "epoch : 91, train_loss : 2.871605634689331\n",
      "epoch : 92, train_loss : 2.871448516845703\n",
      "epoch : 93, train_loss : 2.8712892532348633\n",
      "epoch : 94, train_loss : 2.871124029159546\n",
      "epoch : 95, train_loss : 2.8709535598754883\n",
      "epoch : 96, train_loss : 2.8707778453826904\n",
      "epoch : 97, train_loss : 2.870593547821045\n",
      "epoch : 98, train_loss : 2.8704020977020264\n",
      "epoch : 99, train_loss : 2.8702023029327393\n",
      "epoch : 100, train_loss : 2.869990587234497\n",
      "epoch : 101, train_loss : 2.869767665863037\n",
      "epoch : 102, train_loss : 2.869530200958252\n",
      "epoch : 103, train_loss : 2.8692779541015625\n",
      "epoch : 104, train_loss : 2.869007110595703\n",
      "epoch : 105, train_loss : 2.8687143325805664\n",
      "epoch : 106, train_loss : 2.8683958053588867\n",
      "epoch : 107, train_loss : 2.8680508136749268\n",
      "epoch : 108, train_loss : 2.8676700592041016\n",
      "epoch : 109, train_loss : 2.8672499656677246\n",
      "epoch : 110, train_loss : 2.8667821884155273\n",
      "epoch : 111, train_loss : 2.8662569522857666\n",
      "epoch : 112, train_loss : 2.865661859512329\n",
      "epoch : 113, train_loss : 2.864980936050415\n",
      "epoch : 114, train_loss : 2.864191770553589\n",
      "epoch : 115, train_loss : 2.863264560699463\n",
      "epoch : 116, train_loss : 2.8621582984924316\n",
      "epoch : 117, train_loss : 2.8608126640319824\n",
      "epoch : 118, train_loss : 2.859135389328003\n",
      "epoch : 119, train_loss : 2.8569791316986084\n",
      "epoch : 120, train_loss : 2.854102373123169\n",
      "epoch : 121, train_loss : 2.8500592708587646\n",
      "epoch : 122, train_loss : 2.843967914581299\n",
      "epoch : 123, train_loss : 2.833909511566162\n",
      "epoch : 124, train_loss : 2.8156590461730957\n",
      "epoch : 125, train_loss : 2.787274122238159\n",
      "epoch : 126, train_loss : 2.8312220573425293\n",
      "epoch : 127, train_loss : 3.1616463661193848\n",
      "epoch : 128, train_loss : 3.104630470275879\n",
      "epoch : 129, train_loss : 3.0687763690948486\n",
      "epoch : 130, train_loss : 3.1220624446868896\n",
      "epoch : 131, train_loss : 3.103968620300293\n",
      "epoch : 132, train_loss : 3.112200975418091\n",
      "epoch : 133, train_loss : 3.1151676177978516\n",
      "epoch : 134, train_loss : 3.1148569583892822\n",
      "epoch : 135, train_loss : 3.1164939403533936\n",
      "epoch : 136, train_loss : 3.116075277328491\n",
      "epoch : 137, train_loss : 3.1157567501068115\n",
      "epoch : 138, train_loss : 3.1149754524230957\n",
      "epoch : 139, train_loss : 3.1140384674072266\n",
      "epoch : 140, train_loss : 3.113142490386963\n",
      "epoch : 141, train_loss : 3.1123950481414795\n",
      "epoch : 142, train_loss : 3.1119401454925537\n",
      "epoch : 143, train_loss : 3.1118342876434326\n",
      "epoch : 144, train_loss : 3.1120967864990234\n",
      "epoch : 145, train_loss : 3.1126999855041504\n",
      "epoch : 146, train_loss : 3.1135973930358887\n",
      "epoch : 147, train_loss : 3.114733934402466\n",
      "epoch : 148, train_loss : 3.116065502166748\n",
      "epoch : 149, train_loss : 3.1175622940063477\n",
      "epoch : 150, train_loss : 3.119201898574829\n",
      "epoch : 151, train_loss : 3.120981216430664\n",
      "epoch : 152, train_loss : 3.122905969619751\n",
      "epoch : 153, train_loss : 3.1249914169311523\n",
      "epoch : 154, train_loss : 3.1272616386413574\n",
      "epoch : 155, train_loss : 3.129741668701172\n",
      "epoch : 156, train_loss : 3.132472276687622\n",
      "epoch : 157, train_loss : 3.135499954223633\n",
      "epoch : 158, train_loss : 3.1388790607452393\n",
      "epoch : 159, train_loss : 3.1426775455474854\n",
      "epoch : 160, train_loss : 3.1469690799713135\n",
      "epoch : 161, train_loss : 3.151843786239624\n",
      "epoch : 162, train_loss : 3.1573939323425293\n",
      "epoch : 163, train_loss : 3.1637253761291504\n",
      "epoch : 164, train_loss : 3.170919418334961\n",
      "epoch : 165, train_loss : 3.1790390014648438\n",
      "epoch : 166, train_loss : 3.188096761703491\n",
      "epoch : 167, train_loss : 3.1980128288269043\n",
      "epoch : 168, train_loss : 3.208604335784912\n",
      "epoch : 169, train_loss : 3.2195873260498047\n",
      "epoch : 170, train_loss : 3.230583906173706\n",
      "epoch : 171, train_loss : 3.241180181503296\n",
      "epoch : 172, train_loss : 3.2509820461273193\n",
      "epoch : 173, train_loss : 3.2596819400787354\n",
      "epoch : 174, train_loss : 3.267070770263672\n",
      "epoch : 175, train_loss : 3.273056745529175\n",
      "epoch : 176, train_loss : 3.27764630317688\n",
      "epoch : 177, train_loss : 3.280898094177246\n",
      "epoch : 178, train_loss : 3.2829158306121826\n",
      "epoch : 179, train_loss : 3.283792495727539\n",
      "epoch : 180, train_loss : 3.283616304397583\n",
      "epoch : 181, train_loss : 3.2824454307556152\n",
      "epoch : 182, train_loss : 3.2803080081939697\n",
      "epoch : 183, train_loss : 3.277182102203369\n",
      "epoch : 184, train_loss : 3.2730233669281006\n",
      "epoch : 185, train_loss : 3.2677316665649414\n",
      "epoch : 186, train_loss : 3.261168956756592\n",
      "epoch : 187, train_loss : 3.2531521320343018\n",
      "epoch : 188, train_loss : 3.2434558868408203\n",
      "epoch : 189, train_loss : 3.2318177223205566\n",
      "epoch : 190, train_loss : 3.2179343700408936\n",
      "epoch : 191, train_loss : 3.201496124267578\n",
      "epoch : 192, train_loss : 3.1821930408477783\n",
      "epoch : 193, train_loss : 3.1597700119018555\n",
      "epoch : 194, train_loss : 3.1340742111206055\n",
      "epoch : 195, train_loss : 3.1051008701324463\n",
      "epoch : 196, train_loss : 3.0730669498443604\n",
      "epoch : 197, train_loss : 3.0384230613708496\n",
      "epoch : 198, train_loss : 3.001863718032837\n",
      "epoch : 199, train_loss : 2.9642701148986816\n",
      "epoch : 200, train_loss : 2.926614999771118\n",
      "epoch : 201, train_loss : 2.8898448944091797\n",
      "epoch : 202, train_loss : 2.854773759841919\n",
      "epoch : 203, train_loss : 2.8220133781433105\n",
      "epoch : 204, train_loss : 2.7919373512268066\n",
      "epoch : 205, train_loss : 2.7646942138671875\n",
      "epoch : 206, train_loss : 2.7402498722076416\n",
      "epoch : 207, train_loss : 2.7184367179870605\n",
      "epoch : 208, train_loss : 2.6990058422088623\n",
      "epoch : 209, train_loss : 2.681668758392334\n",
      "epoch : 210, train_loss : 2.6661312580108643\n",
      "epoch : 211, train_loss : 2.652117967605591\n",
      "epoch : 212, train_loss : 2.6393768787384033\n",
      "epoch : 213, train_loss : 2.6276912689208984\n",
      "epoch : 214, train_loss : 2.616882801055908\n",
      "epoch : 215, train_loss : 2.606799602508545\n",
      "epoch : 216, train_loss : 2.597323179244995\n",
      "epoch : 217, train_loss : 2.588360071182251\n",
      "epoch : 218, train_loss : 2.5798354148864746\n",
      "epoch : 219, train_loss : 2.571690797805786\n",
      "epoch : 220, train_loss : 2.563882827758789\n",
      "epoch : 221, train_loss : 2.5563759803771973\n",
      "epoch : 222, train_loss : 2.549142599105835\n",
      "epoch : 223, train_loss : 2.5421648025512695\n",
      "epoch : 224, train_loss : 2.53542160987854\n",
      "epoch : 225, train_loss : 2.5288984775543213\n",
      "epoch : 226, train_loss : 2.5225844383239746\n",
      "epoch : 227, train_loss : 2.5164692401885986\n",
      "epoch : 228, train_loss : 2.51054310798645\n",
      "epoch : 229, train_loss : 2.5047972202301025\n",
      "epoch : 230, train_loss : 2.49922513961792\n",
      "epoch : 231, train_loss : 2.4938158988952637\n",
      "epoch : 232, train_loss : 2.4885663986206055\n",
      "epoch : 233, train_loss : 2.483468770980835\n",
      "epoch : 234, train_loss : 2.478517532348633\n",
      "epoch : 235, train_loss : 2.4737067222595215\n",
      "epoch : 236, train_loss : 2.469031810760498\n",
      "epoch : 237, train_loss : 2.464484214782715\n",
      "epoch : 238, train_loss : 2.460063934326172\n",
      "epoch : 239, train_loss : 2.455763339996338\n",
      "epoch : 240, train_loss : 2.4515774250030518\n",
      "epoch : 241, train_loss : 2.4475040435791016\n",
      "epoch : 242, train_loss : 2.443537473678589\n",
      "epoch : 243, train_loss : 2.439673662185669\n",
      "epoch : 244, train_loss : 2.435911178588867\n",
      "epoch : 245, train_loss : 2.4322426319122314\n",
      "epoch : 246, train_loss : 2.4286692142486572\n",
      "epoch : 247, train_loss : 2.425184965133667\n",
      "epoch : 248, train_loss : 2.4217872619628906\n",
      "epoch : 249, train_loss : 2.4184749126434326\n",
      "epoch : 250, train_loss : 2.41524076461792\n",
      "epoch : 251, train_loss : 2.4120874404907227\n",
      "epoch : 252, train_loss : 2.4090073108673096\n",
      "epoch : 253, train_loss : 2.406001329421997\n",
      "epoch : 254, train_loss : 2.403064012527466\n",
      "epoch : 255, train_loss : 2.4001967906951904\n",
      "epoch : 256, train_loss : 2.3973946571350098\n",
      "epoch : 257, train_loss : 2.394658327102661\n",
      "epoch : 258, train_loss : 2.391981601715088\n",
      "epoch : 259, train_loss : 2.3893675804138184\n",
      "epoch : 260, train_loss : 2.386810064315796\n",
      "epoch : 261, train_loss : 2.3843092918395996\n",
      "epoch : 262, train_loss : 2.381864309310913\n",
      "epoch : 263, train_loss : 2.379472494125366\n",
      "epoch : 264, train_loss : 2.3771307468414307\n",
      "epoch : 265, train_loss : 2.3748395442962646\n",
      "epoch : 266, train_loss : 2.372596502304077\n",
      "epoch : 267, train_loss : 2.370398759841919\n",
      "epoch : 268, train_loss : 2.36824893951416\n",
      "epoch : 269, train_loss : 2.3661417961120605\n",
      "epoch : 270, train_loss : 2.364077568054199\n",
      "epoch : 271, train_loss : 2.3620550632476807\n",
      "epoch : 272, train_loss : 2.3600735664367676\n",
      "epoch : 273, train_loss : 2.3581314086914062\n",
      "epoch : 274, train_loss : 2.356226921081543\n",
      "epoch : 275, train_loss : 2.3543591499328613\n",
      "epoch : 276, train_loss : 2.352527379989624\n",
      "epoch : 277, train_loss : 2.35073184967041\n",
      "epoch : 278, train_loss : 2.348968982696533\n",
      "epoch : 279, train_loss : 2.3472392559051514\n",
      "epoch : 280, train_loss : 2.345543384552002\n",
      "epoch : 281, train_loss : 2.3438773155212402\n",
      "epoch : 282, train_loss : 2.3422420024871826\n",
      "epoch : 283, train_loss : 2.340636730194092\n",
      "epoch : 284, train_loss : 2.339059352874756\n",
      "epoch : 285, train_loss : 2.337510347366333\n",
      "epoch : 286, train_loss : 2.335989236831665\n",
      "epoch : 287, train_loss : 2.3344948291778564\n",
      "epoch : 288, train_loss : 2.333024740219116\n",
      "epoch : 289, train_loss : 2.3315815925598145\n",
      "epoch : 290, train_loss : 2.330162286758423\n",
      "epoch : 291, train_loss : 2.328768253326416\n",
      "epoch : 292, train_loss : 2.327396869659424\n",
      "epoch : 293, train_loss : 2.326047897338867\n",
      "epoch : 294, train_loss : 2.324720621109009\n",
      "epoch : 295, train_loss : 2.323415517807007\n",
      "epoch : 296, train_loss : 2.3221309185028076\n",
      "epoch : 297, train_loss : 2.3208673000335693\n",
      "epoch : 298, train_loss : 2.319624423980713\n",
      "epoch : 299, train_loss : 2.318401575088501\n",
      "epoch : 300, train_loss : 2.317196846008301\n",
      "epoch : 301, train_loss : 2.316009998321533\n",
      "epoch : 302, train_loss : 2.314842700958252\n",
      "epoch : 303, train_loss : 2.313692569732666\n",
      "epoch : 304, train_loss : 2.312559127807617\n",
      "epoch : 305, train_loss : 2.3114452362060547\n",
      "epoch : 306, train_loss : 2.3103456497192383\n",
      "epoch : 307, train_loss : 2.309263229370117\n",
      "epoch : 308, train_loss : 2.3081960678100586\n",
      "epoch : 309, train_loss : 2.307145833969116\n",
      "epoch : 310, train_loss : 2.3061091899871826\n",
      "epoch : 311, train_loss : 2.3050878047943115\n",
      "epoch : 312, train_loss : 2.3040807247161865\n",
      "epoch : 313, train_loss : 2.3030881881713867\n",
      "epoch : 314, train_loss : 2.302107572555542\n",
      "epoch : 315, train_loss : 2.301142930984497\n",
      "epoch : 316, train_loss : 2.3001909255981445\n",
      "epoch : 317, train_loss : 2.2992520332336426\n",
      "epoch : 318, train_loss : 2.2983248233795166\n",
      "epoch : 319, train_loss : 2.2974116802215576\n",
      "epoch : 320, train_loss : 2.2965097427368164\n",
      "epoch : 321, train_loss : 2.2956197261810303\n",
      "epoch : 322, train_loss : 2.294741153717041\n",
      "epoch : 323, train_loss : 2.293875217437744\n",
      "epoch : 324, train_loss : 2.293018102645874\n",
      "epoch : 325, train_loss : 2.292172431945801\n",
      "epoch : 326, train_loss : 2.29133939743042\n",
      "epoch : 327, train_loss : 2.2905168533325195\n",
      "epoch : 328, train_loss : 2.289703845977783\n",
      "epoch : 329, train_loss : 2.28890061378479\n",
      "epoch : 330, train_loss : 2.2881078720092773\n",
      "epoch : 331, train_loss : 2.287324905395508\n",
      "epoch : 332, train_loss : 2.2865517139434814\n",
      "epoch : 333, train_loss : 2.2857882976531982\n",
      "epoch : 334, train_loss : 2.285032272338867\n",
      "epoch : 335, train_loss : 2.284287929534912\n",
      "epoch : 336, train_loss : 2.283550262451172\n",
      "epoch : 337, train_loss : 2.2828214168548584\n",
      "epoch : 338, train_loss : 2.282102108001709\n",
      "epoch : 339, train_loss : 2.28139066696167\n",
      "epoch : 340, train_loss : 2.2806878089904785\n",
      "epoch : 341, train_loss : 2.2799923419952393\n",
      "epoch : 342, train_loss : 2.2793049812316895\n",
      "epoch : 343, train_loss : 2.278625726699829\n",
      "epoch : 344, train_loss : 2.2779548168182373\n",
      "epoch : 345, train_loss : 2.2772903442382812\n",
      "epoch : 346, train_loss : 2.2766337394714355\n",
      "epoch : 347, train_loss : 2.2759850025177\n",
      "epoch : 348, train_loss : 2.2753427028656006\n",
      "epoch : 349, train_loss : 2.2747080326080322\n",
      "epoch : 350, train_loss : 2.274078845977783\n",
      "epoch : 351, train_loss : 2.273458242416382\n",
      "epoch : 352, train_loss : 2.272844076156616\n",
      "epoch : 353, train_loss : 2.272235870361328\n",
      "epoch : 354, train_loss : 2.271634578704834\n",
      "epoch : 355, train_loss : 2.271038293838501\n",
      "epoch : 356, train_loss : 2.270449638366699\n",
      "epoch : 357, train_loss : 2.2698676586151123\n",
      "epoch : 358, train_loss : 2.26928973197937\n",
      "epoch : 359, train_loss : 2.2687199115753174\n",
      "epoch : 360, train_loss : 2.268155574798584\n",
      "epoch : 361, train_loss : 2.267596960067749\n",
      "epoch : 362, train_loss : 2.267042875289917\n",
      "epoch : 363, train_loss : 2.2664947509765625\n",
      "epoch : 364, train_loss : 2.265953540802002\n",
      "epoch : 365, train_loss : 2.265415668487549\n",
      "epoch : 366, train_loss : 2.264885187149048\n",
      "epoch : 367, train_loss : 2.2643580436706543\n",
      "epoch : 368, train_loss : 2.2638375759124756\n",
      "epoch : 369, train_loss : 2.263322353363037\n",
      "epoch : 370, train_loss : 2.2628114223480225\n",
      "epoch : 371, train_loss : 2.262305974960327\n",
      "epoch : 372, train_loss : 2.2618041038513184\n",
      "epoch : 373, train_loss : 2.2613086700439453\n",
      "epoch : 374, train_loss : 2.260817527770996\n",
      "epoch : 375, train_loss : 2.2603304386138916\n",
      "epoch : 376, train_loss : 2.2598483562469482\n",
      "epoch : 377, train_loss : 2.2593703269958496\n",
      "epoch : 378, train_loss : 2.2588982582092285\n",
      "epoch : 379, train_loss : 2.2584285736083984\n",
      "epoch : 380, train_loss : 2.257964611053467\n",
      "epoch : 381, train_loss : 2.25750470161438\n",
      "epoch : 382, train_loss : 2.2570481300354004\n",
      "epoch : 383, train_loss : 2.256596088409424\n",
      "epoch : 384, train_loss : 2.25614857673645\n",
      "epoch : 385, train_loss : 2.255704641342163\n",
      "epoch : 386, train_loss : 2.2552645206451416\n",
      "epoch : 387, train_loss : 2.2548296451568604\n",
      "epoch : 388, train_loss : 2.2543978691101074\n",
      "epoch : 389, train_loss : 2.2539689540863037\n",
      "epoch : 390, train_loss : 2.2535452842712402\n",
      "epoch : 391, train_loss : 2.253124952316284\n",
      "epoch : 392, train_loss : 2.2527081966400146\n",
      "epoch : 393, train_loss : 2.2522943019866943\n",
      "epoch : 394, train_loss : 2.2518844604492188\n",
      "epoch : 395, train_loss : 2.251478672027588\n",
      "epoch : 396, train_loss : 2.2510757446289062\n",
      "epoch : 397, train_loss : 2.2506775856018066\n",
      "epoch : 398, train_loss : 2.2502808570861816\n",
      "epoch : 399, train_loss : 2.249889373779297\n",
      "epoch : 400, train_loss : 2.2494993209838867\n",
      "epoch : 401, train_loss : 2.249112844467163\n",
      "epoch : 402, train_loss : 2.2487306594848633\n",
      "epoch : 403, train_loss : 2.2483506202697754\n",
      "epoch : 404, train_loss : 2.247973680496216\n",
      "epoch : 405, train_loss : 2.247600555419922\n",
      "epoch : 406, train_loss : 2.2472293376922607\n",
      "epoch : 407, train_loss : 2.2468619346618652\n",
      "epoch : 408, train_loss : 2.24649715423584\n",
      "epoch : 409, train_loss : 2.2461347579956055\n",
      "epoch : 410, train_loss : 2.245776414871216\n",
      "epoch : 411, train_loss : 2.24541974067688\n",
      "epoch : 412, train_loss : 2.2450664043426514\n",
      "epoch : 413, train_loss : 2.244715452194214\n",
      "epoch : 414, train_loss : 2.244367837905884\n",
      "epoch : 415, train_loss : 2.2440223693847656\n",
      "epoch : 416, train_loss : 2.243680238723755\n",
      "epoch : 417, train_loss : 2.243339776992798\n",
      "epoch : 418, train_loss : 2.243001937866211\n",
      "epoch : 419, train_loss : 2.2426671981811523\n",
      "epoch : 420, train_loss : 2.242335081100464\n",
      "epoch : 421, train_loss : 2.2420051097869873\n",
      "epoch : 422, train_loss : 2.2416770458221436\n",
      "epoch : 423, train_loss : 2.241351366043091\n",
      "epoch : 424, train_loss : 2.2410290241241455\n",
      "epoch : 425, train_loss : 2.240708112716675\n",
      "epoch : 426, train_loss : 2.2403900623321533\n",
      "epoch : 427, train_loss : 2.240072727203369\n",
      "epoch : 428, train_loss : 2.2397587299346924\n",
      "epoch : 429, train_loss : 2.239447832107544\n",
      "epoch : 430, train_loss : 2.239138603210449\n",
      "epoch : 431, train_loss : 2.238830327987671\n",
      "epoch : 432, train_loss : 2.238525390625\n",
      "epoch : 433, train_loss : 2.2382209300994873\n",
      "epoch : 434, train_loss : 2.2379207611083984\n",
      "epoch : 435, train_loss : 2.2376205921173096\n",
      "epoch : 436, train_loss : 2.237323045730591\n",
      "epoch : 437, train_loss : 2.237027645111084\n",
      "epoch : 438, train_loss : 2.2367336750030518\n",
      "epoch : 439, train_loss : 2.2364416122436523\n",
      "epoch : 440, train_loss : 2.236152172088623\n",
      "epoch : 441, train_loss : 2.235863208770752\n",
      "epoch : 442, train_loss : 2.235577344894409\n",
      "epoch : 443, train_loss : 2.2352921962738037\n",
      "epoch : 444, train_loss : 2.235008478164673\n",
      "epoch : 445, train_loss : 2.2347280979156494\n",
      "epoch : 446, train_loss : 2.2344470024108887\n",
      "epoch : 447, train_loss : 2.2341692447662354\n",
      "epoch : 448, train_loss : 2.2338926792144775\n",
      "epoch : 449, train_loss : 2.233617067337036\n",
      "epoch : 450, train_loss : 2.2333438396453857\n",
      "epoch : 451, train_loss : 2.2330710887908936\n",
      "epoch : 452, train_loss : 2.232800006866455\n",
      "epoch : 453, train_loss : 2.2325305938720703\n",
      "epoch : 454, train_loss : 2.23226261138916\n",
      "epoch : 455, train_loss : 2.2319955825805664\n",
      "epoch : 456, train_loss : 2.2317299842834473\n",
      "epoch : 457, train_loss : 2.2314653396606445\n",
      "epoch : 458, train_loss : 2.231203317642212\n",
      "epoch : 459, train_loss : 2.230940103530884\n",
      "epoch : 460, train_loss : 2.2306792736053467\n",
      "epoch : 461, train_loss : 2.230419635772705\n",
      "epoch : 462, train_loss : 2.230160713195801\n",
      "epoch : 463, train_loss : 2.2299039363861084\n",
      "epoch : 464, train_loss : 2.2296459674835205\n",
      "epoch : 465, train_loss : 2.2293901443481445\n",
      "epoch : 466, train_loss : 2.2291345596313477\n",
      "epoch : 467, train_loss : 2.2288801670074463\n",
      "epoch : 468, train_loss : 2.2286274433135986\n",
      "epoch : 469, train_loss : 2.228374719619751\n",
      "epoch : 470, train_loss : 2.2281229496002197\n",
      "epoch : 471, train_loss : 2.2278711795806885\n",
      "epoch : 472, train_loss : 2.2276203632354736\n",
      "epoch : 473, train_loss : 2.227369785308838\n",
      "epoch : 474, train_loss : 2.227119207382202\n",
      "epoch : 475, train_loss : 2.226870059967041\n",
      "epoch : 476, train_loss : 2.226620674133301\n",
      "epoch : 477, train_loss : 2.2263712882995605\n",
      "epoch : 478, train_loss : 2.2261219024658203\n",
      "epoch : 479, train_loss : 2.225872755050659\n",
      "epoch : 480, train_loss : 2.2256243228912354\n",
      "epoch : 481, train_loss : 2.2253756523132324\n",
      "epoch : 482, train_loss : 2.2251267433166504\n",
      "epoch : 483, train_loss : 2.224877119064331\n",
      "epoch : 484, train_loss : 2.224627733230591\n",
      "epoch : 485, train_loss : 2.2243776321411133\n",
      "epoch : 486, train_loss : 2.2241265773773193\n",
      "epoch : 487, train_loss : 2.2238762378692627\n",
      "epoch : 488, train_loss : 2.2236242294311523\n",
      "epoch : 489, train_loss : 2.2233715057373047\n",
      "epoch : 490, train_loss : 2.2231178283691406\n",
      "epoch : 491, train_loss : 2.222862720489502\n",
      "epoch : 492, train_loss : 2.2226061820983887\n",
      "epoch : 493, train_loss : 2.22234845161438\n",
      "epoch : 494, train_loss : 2.2220895290374756\n",
      "epoch : 495, train_loss : 2.2218282222747803\n",
      "epoch : 496, train_loss : 2.221565008163452\n",
      "epoch : 497, train_loss : 2.221299171447754\n",
      "epoch : 498, train_loss : 2.2210309505462646\n",
      "epoch : 499, train_loss : 2.2207608222961426\n",
      "epoch : 500, train_loss : 2.220486879348755\n",
      "epoch : 501, train_loss : 2.220209836959839\n",
      "epoch : 502, train_loss : 2.219928741455078\n",
      "epoch : 503, train_loss : 2.219644546508789\n",
      "epoch : 504, train_loss : 2.219355821609497\n",
      "epoch : 505, train_loss : 2.2190616130828857\n",
      "epoch : 506, train_loss : 2.218761444091797\n",
      "epoch : 507, train_loss : 2.2184574604034424\n",
      "epoch : 508, train_loss : 2.2181458473205566\n",
      "epoch : 509, train_loss : 2.2178287506103516\n",
      "epoch : 510, train_loss : 2.2175028324127197\n",
      "epoch : 511, train_loss : 2.217168092727661\n",
      "epoch : 512, train_loss : 2.216824531555176\n",
      "epoch : 513, train_loss : 2.2164711952209473\n",
      "epoch : 514, train_loss : 2.2161061763763428\n",
      "epoch : 515, train_loss : 2.215729236602783\n",
      "epoch : 516, train_loss : 2.2153375148773193\n",
      "epoch : 517, train_loss : 2.2149314880371094\n",
      "epoch : 518, train_loss : 2.214508056640625\n",
      "epoch : 519, train_loss : 2.21406626701355\n",
      "epoch : 520, train_loss : 2.2136025428771973\n",
      "epoch : 521, train_loss : 2.2131149768829346\n",
      "epoch : 522, train_loss : 2.212601661682129\n",
      "epoch : 523, train_loss : 2.212057590484619\n",
      "epoch : 524, train_loss : 2.2114791870117188\n",
      "epoch : 525, train_loss : 2.2108635902404785\n",
      "epoch : 526, train_loss : 2.2102038860321045\n",
      "epoch : 527, train_loss : 2.209493637084961\n",
      "epoch : 528, train_loss : 2.208725929260254\n",
      "epoch : 529, train_loss : 2.207892894744873\n",
      "epoch : 530, train_loss : 2.206983804702759\n",
      "epoch : 531, train_loss : 2.205986499786377\n",
      "epoch : 532, train_loss : 2.204888105392456\n",
      "epoch : 533, train_loss : 2.2036702632904053\n",
      "epoch : 534, train_loss : 2.202314615249634\n",
      "epoch : 535, train_loss : 2.2007970809936523\n",
      "epoch : 536, train_loss : 2.1990933418273926\n",
      "epoch : 537, train_loss : 2.197176218032837\n",
      "epoch : 538, train_loss : 2.1950182914733887\n",
      "epoch : 539, train_loss : 2.1925971508026123\n",
      "epoch : 540, train_loss : 2.189903974533081\n",
      "epoch : 541, train_loss : 2.186962604522705\n",
      "epoch : 542, train_loss : 2.183849811553955\n",
      "epoch : 543, train_loss : 2.180737018585205\n",
      "epoch : 544, train_loss : 2.1779239177703857\n",
      "epoch : 545, train_loss : 2.1758644580841064\n",
      "epoch : 546, train_loss : 2.175138473510742\n",
      "epoch : 547, train_loss : 2.17635178565979\n",
      "epoch : 548, train_loss : 2.1799814701080322\n",
      "epoch : 549, train_loss : 2.186223268508911\n",
      "epoch : 550, train_loss : 2.1949381828308105\n",
      "epoch : 551, train_loss : 2.2056620121002197\n",
      "epoch : 552, train_loss : 2.2177016735076904\n",
      "epoch : 553, train_loss : 2.230207681655884\n",
      "epoch : 554, train_loss : 2.2422468662261963\n",
      "epoch : 555, train_loss : 2.252863645553589\n",
      "epoch : 556, train_loss : 2.261155843734741\n",
      "epoch : 557, train_loss : 2.2663700580596924\n",
      "epoch : 558, train_loss : 2.267991304397583\n",
      "epoch : 559, train_loss : 2.2658169269561768\n",
      "epoch : 560, train_loss : 2.259971857070923\n",
      "epoch : 561, train_loss : 2.2508604526519775\n",
      "epoch : 562, train_loss : 2.239069938659668\n",
      "epoch : 563, train_loss : 2.225252628326416\n",
      "epoch : 564, train_loss : 2.2100296020507812\n",
      "epoch : 565, train_loss : 2.193920373916626\n",
      "epoch : 566, train_loss : 2.1773128509521484\n",
      "epoch : 567, train_loss : 2.160473108291626\n",
      "epoch : 568, train_loss : 2.143558979034424\n",
      "epoch : 569, train_loss : 2.126650810241699\n",
      "epoch : 570, train_loss : 2.1097724437713623\n",
      "epoch : 571, train_loss : 2.0929155349731445\n",
      "epoch : 572, train_loss : 2.076057195663452\n",
      "epoch : 573, train_loss : 2.059166669845581\n",
      "epoch : 574, train_loss : 2.042215347290039\n",
      "epoch : 575, train_loss : 2.0251808166503906\n",
      "epoch : 576, train_loss : 2.008045196533203\n",
      "epoch : 577, train_loss : 1.990799903869629\n",
      "epoch : 578, train_loss : 1.973447322845459\n",
      "epoch : 579, train_loss : 1.9559924602508545\n",
      "epoch : 580, train_loss : 1.9384506940841675\n",
      "epoch : 581, train_loss : 1.9208425283432007\n",
      "epoch : 582, train_loss : 1.9031949043273926\n",
      "epoch : 583, train_loss : 1.8855398893356323\n",
      "epoch : 584, train_loss : 1.8679131269454956\n",
      "epoch : 585, train_loss : 1.8503527641296387\n",
      "epoch : 586, train_loss : 1.8329017162322998\n",
      "epoch : 587, train_loss : 1.8156019449234009\n",
      "epoch : 588, train_loss : 1.798488974571228\n",
      "epoch : 589, train_loss : 1.7816014289855957\n",
      "epoch : 590, train_loss : 1.7649708986282349\n",
      "epoch : 591, train_loss : 1.7486200332641602\n",
      "epoch : 592, train_loss : 1.7325694561004639\n",
      "epoch : 593, train_loss : 1.7168325185775757\n",
      "epoch : 594, train_loss : 1.7014148235321045\n",
      "epoch : 595, train_loss : 1.686320185661316\n",
      "epoch : 596, train_loss : 1.6715459823608398\n",
      "epoch : 597, train_loss : 1.6570892333984375\n",
      "epoch : 598, train_loss : 1.6429437398910522\n",
      "epoch : 599, train_loss : 1.6291011571884155\n",
      "epoch : 600, train_loss : 1.615553379058838\n",
      "epoch : 601, train_loss : 1.6022934913635254\n",
      "epoch : 602, train_loss : 1.589313268661499\n",
      "epoch : 603, train_loss : 1.5766040086746216\n",
      "epoch : 604, train_loss : 1.5641587972640991\n",
      "epoch : 605, train_loss : 1.5519715547561646\n",
      "epoch : 606, train_loss : 1.5400357246398926\n",
      "epoch : 607, train_loss : 1.528346300125122\n",
      "epoch : 608, train_loss : 1.516897201538086\n",
      "epoch : 609, train_loss : 1.5056840181350708\n",
      "epoch : 610, train_loss : 1.4947012662887573\n",
      "epoch : 611, train_loss : 1.4839460849761963\n",
      "epoch : 612, train_loss : 1.4734143018722534\n",
      "epoch : 613, train_loss : 1.4631019830703735\n",
      "epoch : 614, train_loss : 1.4530059099197388\n",
      "epoch : 615, train_loss : 1.4431228637695312\n",
      "epoch : 616, train_loss : 1.4334489107131958\n",
      "epoch : 617, train_loss : 1.4239816665649414\n",
      "epoch : 618, train_loss : 1.414718747138977\n",
      "epoch : 619, train_loss : 1.405656099319458\n",
      "epoch : 620, train_loss : 1.396790623664856\n",
      "epoch : 621, train_loss : 1.3881206512451172\n",
      "epoch : 622, train_loss : 1.379643201828003\n",
      "epoch : 623, train_loss : 1.3713549375534058\n",
      "epoch : 624, train_loss : 1.3632532358169556\n",
      "epoch : 625, train_loss : 1.3553357124328613\n",
      "epoch : 626, train_loss : 1.3475996255874634\n",
      "epoch : 627, train_loss : 1.340041995048523\n",
      "epoch : 628, train_loss : 1.332660436630249\n",
      "epoch : 629, train_loss : 1.3254526853561401\n",
      "epoch : 630, train_loss : 1.3184162378311157\n",
      "epoch : 631, train_loss : 1.311547040939331\n",
      "epoch : 632, train_loss : 1.304844617843628\n",
      "epoch : 633, train_loss : 1.298305869102478\n",
      "epoch : 634, train_loss : 1.2919270992279053\n",
      "epoch : 635, train_loss : 1.285707712173462\n",
      "epoch : 636, train_loss : 1.279643177986145\n",
      "epoch : 637, train_loss : 1.273734450340271\n",
      "epoch : 638, train_loss : 1.2679765224456787\n",
      "epoch : 639, train_loss : 1.2623677253723145\n",
      "epoch : 640, train_loss : 1.2569059133529663\n",
      "epoch : 641, train_loss : 1.251590371131897\n",
      "epoch : 642, train_loss : 1.2464184761047363\n",
      "epoch : 643, train_loss : 1.2413862943649292\n",
      "epoch : 644, train_loss : 1.2364939451217651\n",
      "epoch : 645, train_loss : 1.2317395210266113\n",
      "epoch : 646, train_loss : 1.227120280265808\n",
      "epoch : 647, train_loss : 1.2226364612579346\n",
      "epoch : 648, train_loss : 1.2182848453521729\n",
      "epoch : 649, train_loss : 1.2140638828277588\n",
      "epoch : 650, train_loss : 1.209973931312561\n",
      "epoch : 651, train_loss : 1.2060126066207886\n",
      "epoch : 652, train_loss : 1.2021784782409668\n",
      "epoch : 653, train_loss : 1.198470950126648\n",
      "epoch : 654, train_loss : 1.1948890686035156\n",
      "epoch : 655, train_loss : 1.191431999206543\n",
      "epoch : 656, train_loss : 1.188099980354309\n",
      "epoch : 657, train_loss : 1.1848912239074707\n",
      "epoch : 658, train_loss : 1.1818054914474487\n",
      "epoch : 659, train_loss : 1.1788432598114014\n",
      "epoch : 660, train_loss : 1.1760046482086182\n",
      "epoch : 661, train_loss : 1.1732882261276245\n",
      "epoch : 662, train_loss : 1.1706949472427368\n",
      "epoch : 663, train_loss : 1.1682263612747192\n",
      "epoch : 664, train_loss : 1.1658815145492554\n",
      "epoch : 665, train_loss : 1.1636615991592407\n",
      "epoch : 666, train_loss : 1.1615686416625977\n",
      "epoch : 667, train_loss : 1.1596031188964844\n",
      "epoch : 668, train_loss : 1.1577651500701904\n",
      "epoch : 669, train_loss : 1.1560583114624023\n",
      "epoch : 670, train_loss : 1.1544839143753052\n",
      "epoch : 671, train_loss : 1.1530436277389526\n",
      "epoch : 672, train_loss : 1.151740312576294\n",
      "epoch : 673, train_loss : 1.1505768299102783\n",
      "epoch : 674, train_loss : 1.149556040763855\n",
      "epoch : 675, train_loss : 1.1486811637878418\n",
      "epoch : 676, train_loss : 1.1479560136795044\n",
      "epoch : 677, train_loss : 1.1473842859268188\n",
      "epoch : 678, train_loss : 1.146971583366394\n",
      "epoch : 679, train_loss : 1.1467211246490479\n",
      "epoch : 680, train_loss : 1.1466387510299683\n",
      "epoch : 681, train_loss : 1.1467299461364746\n",
      "epoch : 682, train_loss : 1.1470009088516235\n",
      "epoch : 683, train_loss : 1.1474577188491821\n",
      "epoch : 684, train_loss : 1.1481077671051025\n",
      "epoch : 685, train_loss : 1.1489588022232056\n",
      "epoch : 686, train_loss : 1.1500166654586792\n",
      "epoch : 687, train_loss : 1.151291012763977\n",
      "epoch : 688, train_loss : 1.1527907848358154\n",
      "epoch : 689, train_loss : 1.1545239686965942\n",
      "epoch : 690, train_loss : 1.1564996242523193\n",
      "epoch : 691, train_loss : 1.1587274074554443\n",
      "epoch : 692, train_loss : 1.1612157821655273\n",
      "epoch : 693, train_loss : 1.1639747619628906\n",
      "epoch : 694, train_loss : 1.1670138835906982\n",
      "epoch : 695, train_loss : 1.170340895652771\n",
      "epoch : 696, train_loss : 1.1739639043807983\n",
      "epoch : 697, train_loss : 1.177891492843628\n",
      "epoch : 698, train_loss : 1.1821272373199463\n",
      "epoch : 699, train_loss : 1.1866755485534668\n",
      "epoch : 700, train_loss : 1.191538691520691\n",
      "epoch : 701, train_loss : 1.196714162826538\n",
      "epoch : 702, train_loss : 1.2021968364715576\n",
      "epoch : 703, train_loss : 1.2079788446426392\n",
      "epoch : 704, train_loss : 1.2140461206436157\n",
      "epoch : 705, train_loss : 1.2203788757324219\n",
      "epoch : 706, train_loss : 1.226950764656067\n",
      "epoch : 707, train_loss : 1.2337299585342407\n",
      "epoch : 708, train_loss : 1.2406749725341797\n",
      "epoch : 709, train_loss : 1.2477387189865112\n",
      "epoch : 710, train_loss : 1.2548660039901733\n",
      "epoch : 711, train_loss : 1.2619943618774414\n",
      "epoch : 712, train_loss : 1.2690552473068237\n",
      "epoch : 713, train_loss : 1.2759733200073242\n",
      "epoch : 714, train_loss : 1.282671332359314\n",
      "epoch : 715, train_loss : 1.2890722751617432\n",
      "epoch : 716, train_loss : 1.2950983047485352\n",
      "epoch : 717, train_loss : 1.3006757497787476\n",
      "epoch : 718, train_loss : 1.305739402770996\n",
      "epoch : 719, train_loss : 1.3102360963821411\n",
      "epoch : 720, train_loss : 1.314122200012207\n",
      "epoch : 721, train_loss : 1.3173727989196777\n",
      "epoch : 722, train_loss : 1.3199796676635742\n",
      "epoch : 723, train_loss : 1.3219494819641113\n",
      "epoch : 724, train_loss : 1.3233065605163574\n",
      "epoch : 725, train_loss : 1.324090600013733\n",
      "epoch : 726, train_loss : 1.324354887008667\n",
      "epoch : 727, train_loss : 1.3241586685180664\n",
      "epoch : 728, train_loss : 1.3235745429992676\n",
      "epoch : 729, train_loss : 1.3226748704910278\n",
      "epoch : 730, train_loss : 1.3215336799621582\n",
      "epoch : 731, train_loss : 1.320224404335022\n",
      "epoch : 732, train_loss : 1.3188114166259766\n",
      "epoch : 733, train_loss : 1.3173565864562988\n",
      "epoch : 734, train_loss : 1.3159149885177612\n",
      "epoch : 735, train_loss : 1.3145313262939453\n",
      "epoch : 736, train_loss : 1.313241958618164\n",
      "epoch : 737, train_loss : 1.3120726346969604\n",
      "epoch : 738, train_loss : 1.311040997505188\n",
      "epoch : 739, train_loss : 1.3101590871810913\n",
      "epoch : 740, train_loss : 1.3094303607940674\n",
      "epoch : 741, train_loss : 1.308851957321167\n",
      "epoch : 742, train_loss : 1.3084148168563843\n",
      "epoch : 743, train_loss : 1.3081084489822388\n",
      "epoch : 744, train_loss : 1.3079180717468262\n",
      "epoch : 745, train_loss : 1.307824730873108\n",
      "epoch : 746, train_loss : 1.3078104257583618\n",
      "epoch : 747, train_loss : 1.3078546524047852\n",
      "epoch : 748, train_loss : 1.3079395294189453\n",
      "epoch : 749, train_loss : 1.308045506477356\n",
      "epoch : 750, train_loss : 1.3081563711166382\n",
      "epoch : 751, train_loss : 1.3082538843154907\n",
      "epoch : 752, train_loss : 1.3083274364471436\n",
      "epoch : 753, train_loss : 1.3083640336990356\n",
      "epoch : 754, train_loss : 1.3083535432815552\n",
      "epoch : 755, train_loss : 1.3082921504974365\n",
      "epoch : 756, train_loss : 1.3081740140914917\n",
      "epoch : 757, train_loss : 1.307997703552246\n",
      "epoch : 758, train_loss : 1.3077623844146729\n",
      "epoch : 759, train_loss : 1.307471513748169\n",
      "epoch : 760, train_loss : 1.3071285486221313\n",
      "epoch : 761, train_loss : 1.306736946105957\n",
      "epoch : 762, train_loss : 1.3063024282455444\n",
      "epoch : 763, train_loss : 1.3058326244354248\n",
      "epoch : 764, train_loss : 1.3053327798843384\n",
      "epoch : 765, train_loss : 1.3048094511032104\n",
      "epoch : 766, train_loss : 1.304269790649414\n",
      "epoch : 767, train_loss : 1.3037190437316895\n",
      "epoch : 768, train_loss : 1.3031631708145142\n",
      "epoch : 769, train_loss : 1.3026071786880493\n",
      "epoch : 770, train_loss : 1.3020541667938232\n",
      "epoch : 771, train_loss : 1.301508903503418\n",
      "epoch : 772, train_loss : 1.3009741306304932\n",
      "epoch : 773, train_loss : 1.3004508018493652\n",
      "epoch : 774, train_loss : 1.2999392747879028\n",
      "epoch : 775, train_loss : 1.2994418144226074\n",
      "epoch : 776, train_loss : 1.2989572286605835\n",
      "epoch : 777, train_loss : 1.2984874248504639\n",
      "epoch : 778, train_loss : 1.2980289459228516\n",
      "epoch : 779, train_loss : 1.2975820302963257\n",
      "epoch : 780, train_loss : 1.2971441745758057\n",
      "epoch : 781, train_loss : 1.296716332435608\n",
      "epoch : 782, train_loss : 1.296295404434204\n",
      "epoch : 783, train_loss : 1.2958797216415405\n",
      "epoch : 784, train_loss : 1.2954686880111694\n",
      "epoch : 785, train_loss : 1.2950623035430908\n",
      "epoch : 786, train_loss : 1.294656753540039\n",
      "epoch : 787, train_loss : 1.294251561164856\n",
      "epoch : 788, train_loss : 1.2938467264175415\n",
      "epoch : 789, train_loss : 1.293441653251648\n",
      "epoch : 790, train_loss : 1.29303777217865\n",
      "epoch : 791, train_loss : 1.2926304340362549\n",
      "epoch : 792, train_loss : 1.2922219038009644\n",
      "epoch : 793, train_loss : 1.2918140888214111\n",
      "epoch : 794, train_loss : 1.2914049625396729\n",
      "epoch : 795, train_loss : 1.2909948825836182\n",
      "epoch : 796, train_loss : 1.2905843257904053\n",
      "epoch : 797, train_loss : 1.2901737689971924\n",
      "epoch : 798, train_loss : 1.289763331413269\n",
      "epoch : 799, train_loss : 1.2893524169921875\n",
      "epoch : 800, train_loss : 1.288943886756897\n",
      "epoch : 801, train_loss : 1.2885366678237915\n",
      "epoch : 802, train_loss : 1.2881308794021606\n",
      "epoch : 803, train_loss : 1.2877260446548462\n",
      "epoch : 804, train_loss : 1.2873250246047974\n",
      "epoch : 805, train_loss : 1.286926507949829\n",
      "epoch : 806, train_loss : 1.2865301370620728\n",
      "epoch : 807, train_loss : 1.2861381769180298\n",
      "epoch : 808, train_loss : 1.2857482433319092\n",
      "epoch : 809, train_loss : 1.2853610515594482\n",
      "epoch : 810, train_loss : 1.2849748134613037\n",
      "epoch : 811, train_loss : 1.284592866897583\n",
      "epoch : 812, train_loss : 1.2842129468917847\n",
      "epoch : 813, train_loss : 1.2838356494903564\n",
      "epoch : 814, train_loss : 1.283460021018982\n",
      "epoch : 815, train_loss : 1.2830876111984253\n",
      "epoch : 816, train_loss : 1.282718539237976\n",
      "epoch : 817, train_loss : 1.2823511362075806\n",
      "epoch : 818, train_loss : 1.281983494758606\n",
      "epoch : 819, train_loss : 1.2816200256347656\n",
      "epoch : 820, train_loss : 1.2812567949295044\n",
      "epoch : 821, train_loss : 1.2808955907821655\n",
      "epoch : 822, train_loss : 1.2805356979370117\n",
      "epoch : 823, train_loss : 1.280177354812622\n",
      "epoch : 824, train_loss : 1.279819369316101\n",
      "epoch : 825, train_loss : 1.2794625759124756\n",
      "epoch : 826, train_loss : 1.279107689857483\n",
      "epoch : 827, train_loss : 1.278753638267517\n",
      "epoch : 828, train_loss : 1.2783993482589722\n",
      "epoch : 829, train_loss : 1.278046727180481\n",
      "epoch : 830, train_loss : 1.2776954174041748\n",
      "epoch : 831, train_loss : 1.2773443460464478\n",
      "epoch : 832, train_loss : 1.2769935131072998\n",
      "epoch : 833, train_loss : 1.2766444683074951\n",
      "epoch : 834, train_loss : 1.2762953042984009\n",
      "epoch : 835, train_loss : 1.2759478092193604\n",
      "epoch : 836, train_loss : 1.275598406791687\n",
      "epoch : 837, train_loss : 1.275251030921936\n",
      "epoch : 838, train_loss : 1.2749031782150269\n",
      "epoch : 839, train_loss : 1.274557113647461\n",
      "epoch : 840, train_loss : 1.2742100954055786\n",
      "epoch : 841, train_loss : 1.273863434791565\n",
      "epoch : 842, train_loss : 1.2735168933868408\n",
      "epoch : 843, train_loss : 1.2731701135635376\n",
      "epoch : 844, train_loss : 1.2728229761123657\n",
      "epoch : 845, train_loss : 1.2724756002426147\n",
      "epoch : 846, train_loss : 1.272127628326416\n",
      "epoch : 847, train_loss : 1.271780252456665\n",
      "epoch : 848, train_loss : 1.2714307308197021\n",
      "epoch : 849, train_loss : 1.2710824012756348\n",
      "epoch : 850, train_loss : 1.2707324028015137\n",
      "epoch : 851, train_loss : 1.2703821659088135\n",
      "epoch : 852, train_loss : 1.2700304985046387\n",
      "epoch : 853, train_loss : 1.2696797847747803\n",
      "epoch : 854, train_loss : 1.269328236579895\n",
      "epoch : 855, train_loss : 1.2689762115478516\n",
      "epoch : 856, train_loss : 1.268621802330017\n",
      "epoch : 857, train_loss : 1.2682678699493408\n",
      "epoch : 858, train_loss : 1.2679117918014526\n",
      "epoch : 859, train_loss : 1.2675549983978271\n",
      "epoch : 860, train_loss : 1.2671974897384644\n",
      "epoch : 861, train_loss : 1.2668391466140747\n",
      "epoch : 862, train_loss : 1.2664780616760254\n",
      "epoch : 863, train_loss : 1.2661163806915283\n",
      "epoch : 864, train_loss : 1.2657538652420044\n",
      "epoch : 865, train_loss : 1.2653896808624268\n",
      "epoch : 866, train_loss : 1.2650232315063477\n",
      "epoch : 867, train_loss : 1.2646573781967163\n",
      "epoch : 868, train_loss : 1.2642887830734253\n",
      "epoch : 869, train_loss : 1.263919472694397\n",
      "epoch : 870, train_loss : 1.2635473012924194\n",
      "epoch : 871, train_loss : 1.2631747722625732\n",
      "epoch : 872, train_loss : 1.262801170349121\n",
      "epoch : 873, train_loss : 1.2624249458312988\n",
      "epoch : 874, train_loss : 1.2620474100112915\n",
      "epoch : 875, train_loss : 1.2616691589355469\n",
      "epoch : 876, train_loss : 1.2612884044647217\n",
      "epoch : 877, train_loss : 1.2609061002731323\n",
      "epoch : 878, train_loss : 1.260521650314331\n",
      "epoch : 879, train_loss : 1.2601360082626343\n",
      "epoch : 880, train_loss : 1.2597479820251465\n",
      "epoch : 881, train_loss : 1.2593594789505005\n",
      "epoch : 882, train_loss : 1.2589683532714844\n",
      "epoch : 883, train_loss : 1.2585742473602295\n",
      "epoch : 884, train_loss : 1.2581795454025269\n",
      "epoch : 885, train_loss : 1.2577824592590332\n",
      "epoch : 886, train_loss : 1.257383942604065\n",
      "epoch : 887, train_loss : 1.2569818496704102\n",
      "epoch : 888, train_loss : 1.256577968597412\n",
      "epoch : 889, train_loss : 1.2561731338500977\n",
      "epoch : 890, train_loss : 1.2557661533355713\n",
      "epoch : 891, train_loss : 1.2553564310073853\n",
      "epoch : 892, train_loss : 1.2549446821212769\n",
      "epoch : 893, train_loss : 1.254531979560852\n",
      "epoch : 894, train_loss : 1.2541171312332153\n",
      "epoch : 895, train_loss : 1.2537007331848145\n",
      "epoch : 896, train_loss : 1.253281593322754\n",
      "epoch : 897, train_loss : 1.2528616189956665\n",
      "epoch : 898, train_loss : 1.2524398565292358\n",
      "epoch : 899, train_loss : 1.2520155906677246\n",
      "epoch : 900, train_loss : 1.2515898942947388\n",
      "epoch : 901, train_loss : 1.2511619329452515\n",
      "epoch : 902, train_loss : 1.2507330179214478\n",
      "epoch : 903, train_loss : 1.250301718711853\n",
      "epoch : 904, train_loss : 1.249869465827942\n",
      "epoch : 905, train_loss : 1.2494348287582397\n",
      "epoch : 906, train_loss : 1.248997449874878\n",
      "epoch : 907, train_loss : 1.2485582828521729\n",
      "epoch : 908, train_loss : 1.2481173276901245\n",
      "epoch : 909, train_loss : 1.2476743459701538\n",
      "epoch : 910, train_loss : 1.2472304105758667\n",
      "epoch : 911, train_loss : 1.246783971786499\n",
      "epoch : 912, train_loss : 1.2463363409042358\n",
      "epoch : 913, train_loss : 1.2458863258361816\n",
      "epoch : 914, train_loss : 1.2454358339309692\n",
      "epoch : 915, train_loss : 1.2449837923049927\n",
      "epoch : 916, train_loss : 1.244530200958252\n",
      "epoch : 917, train_loss : 1.244076132774353\n",
      "epoch : 918, train_loss : 1.2436202764511108\n",
      "epoch : 919, train_loss : 1.2431632280349731\n",
      "epoch : 920, train_loss : 1.2427043914794922\n",
      "epoch : 921, train_loss : 1.2422456741333008\n",
      "epoch : 922, train_loss : 1.2417857646942139\n",
      "epoch : 923, train_loss : 1.2413235902786255\n",
      "epoch : 924, train_loss : 1.2408604621887207\n",
      "epoch : 925, train_loss : 1.24039626121521\n",
      "epoch : 926, train_loss : 1.2399319410324097\n",
      "epoch : 927, train_loss : 1.2394673824310303\n",
      "epoch : 928, train_loss : 1.2390005588531494\n",
      "epoch : 929, train_loss : 1.2385345697402954\n",
      "epoch : 930, train_loss : 1.2380684614181519\n",
      "epoch : 931, train_loss : 1.2376011610031128\n",
      "epoch : 932, train_loss : 1.23713219165802\n",
      "epoch : 933, train_loss : 1.2366645336151123\n",
      "epoch : 934, train_loss : 1.2361968755722046\n",
      "epoch : 935, train_loss : 1.2357280254364014\n",
      "epoch : 936, train_loss : 1.2352598905563354\n",
      "epoch : 937, train_loss : 1.234791874885559\n",
      "epoch : 938, train_loss : 1.2343240976333618\n",
      "epoch : 939, train_loss : 1.2338587045669556\n",
      "epoch : 940, train_loss : 1.233391523361206\n",
      "epoch : 941, train_loss : 1.2329262495040894\n",
      "epoch : 942, train_loss : 1.2324604988098145\n",
      "epoch : 943, train_loss : 1.2319961786270142\n",
      "epoch : 944, train_loss : 1.2315322160720825\n",
      "epoch : 945, train_loss : 1.231069803237915\n",
      "epoch : 946, train_loss : 1.2306079864501953\n",
      "epoch : 947, train_loss : 1.2301470041275024\n",
      "epoch : 948, train_loss : 1.2296885251998901\n",
      "epoch : 949, train_loss : 1.2292307615280151\n",
      "epoch : 950, train_loss : 1.2287746667861938\n",
      "epoch : 951, train_loss : 1.2283198833465576\n",
      "epoch : 952, train_loss : 1.227866768836975\n",
      "epoch : 953, train_loss : 1.227415680885315\n",
      "epoch : 954, train_loss : 1.2269666194915771\n",
      "epoch : 955, train_loss : 1.2265185117721558\n",
      "epoch : 956, train_loss : 1.2260732650756836\n",
      "epoch : 957, train_loss : 1.2256299257278442\n",
      "epoch : 958, train_loss : 1.2251886129379272\n",
      "epoch : 959, train_loss : 1.2247501611709595\n",
      "epoch : 960, train_loss : 1.2243142127990723\n",
      "epoch : 961, train_loss : 1.2238799333572388\n",
      "epoch : 962, train_loss : 1.2234492301940918\n",
      "epoch : 963, train_loss : 1.2230216264724731\n",
      "epoch : 964, train_loss : 1.222596287727356\n",
      "epoch : 965, train_loss : 1.2221735715866089\n",
      "epoch : 966, train_loss : 1.221754550933838\n",
      "epoch : 967, train_loss : 1.2213377952575684\n",
      "epoch : 968, train_loss : 1.2209237813949585\n",
      "epoch : 969, train_loss : 1.2205126285552979\n",
      "epoch : 970, train_loss : 1.220105528831482\n",
      "epoch : 971, train_loss : 1.2197022438049316\n",
      "epoch : 972, train_loss : 1.2193008661270142\n",
      "epoch : 973, train_loss : 1.2189037799835205\n",
      "epoch : 974, train_loss : 1.2185096740722656\n",
      "epoch : 975, train_loss : 1.2181183099746704\n",
      "epoch : 976, train_loss : 1.2177304029464722\n",
      "epoch : 977, train_loss : 1.217347502708435\n",
      "epoch : 978, train_loss : 1.2169667482376099\n",
      "epoch : 979, train_loss : 1.2165907621383667\n",
      "epoch : 980, train_loss : 1.216219425201416\n",
      "epoch : 981, train_loss : 1.215850591659546\n",
      "epoch : 982, train_loss : 1.2154850959777832\n",
      "epoch : 983, train_loss : 1.2151250839233398\n",
      "epoch : 984, train_loss : 1.214768409729004\n",
      "epoch : 985, train_loss : 1.2144153118133545\n",
      "epoch : 986, train_loss : 1.2140660285949707\n",
      "epoch : 987, train_loss : 1.2137209177017212\n",
      "epoch : 988, train_loss : 1.2133787870407104\n",
      "epoch : 989, train_loss : 1.2130404710769653\n",
      "epoch : 990, train_loss : 1.2127058506011963\n",
      "epoch : 991, train_loss : 1.2123768329620361\n",
      "epoch : 992, train_loss : 1.2120498418807983\n",
      "epoch : 993, train_loss : 1.2117273807525635\n",
      "epoch : 994, train_loss : 1.2114088535308838\n",
      "epoch : 995, train_loss : 1.211094617843628\n",
      "epoch : 996, train_loss : 1.210784673690796\n",
      "epoch : 997, train_loss : 1.210479736328125\n",
      "epoch : 998, train_loss : 1.2101775407791138\n",
      "epoch : 999, train_loss : 1.2098796367645264\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ArrayRef: invalid index Index = 18446744073709551615; Length = 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m X_val[i]\n\u001b[1;32m    109\u001b[0m label \u001b[38;5;241m=\u001b[39m y_val[i]\n\u001b[0;32m--> 110\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output,label\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    112\u001b[0m test_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 46\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) :\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ArrayRef: invalid index Index = 18446744073709551615; Length = 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "alpha = 0.1\n",
    "K = 1000\n",
    "B = 128\n",
    "N = 512\n",
    "\n",
    "def f_true(x) :\n",
    "    return (x-2) * np.cos(x*4)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X_train = torch.normal(0.0, 1.0, (N,))\n",
    "y_train = f_true(X_train)\n",
    "X_val = torch.normal(0.0, 1.0, (N//5,))\n",
    "y_val = f_true(X_val)\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=B)\n",
    "test_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=B)\n",
    "\n",
    "'''\n",
    "unsqueeze(1) reshapes the data into dimension [N,1],\n",
    "where is 1 the dimension of an data point.\n",
    "\n",
    "The batchsize of the test dataloader should not affect the test result\n",
    "so setting batch_size=N may simplify your code.\n",
    "In practice, however, the batchsize for the training dataloader\n",
    "is usually chosen to be as large as possible while not exceeding\n",
    "the memory size of the GPU. In such cases, it is not possible to\n",
    "use a larger batchsize for the test dataloader.\n",
    "'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=1) :\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64, bias=True)\n",
    "        self.linear2 = nn.Linear(64, 64, bias=True)\n",
    "        self.linear3 = nn.Linear(64, 1 , bias=True)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "model = MLP()\n",
    "\n",
    "model.linear1.weight.data = torch.normal (0 , 1 , model.linear1.weight.shape)\n",
    "model.linear1.bias.data = torch.full(model.linear1.bias.shape, 0.03)\n",
    "\n",
    "model.linear2.weight.data = torch.normal (0 , 1 , model.linear2.weight.shape)\n",
    "model.linear2.bias.data = torch.full(model.linear2.bias.shape, 0.03)\n",
    "\n",
    "model.linear3.weight.data = torch.normal (0 , 1 , model.linear3.weight.shape)\n",
    "model.linear3.bias.data = torch.full(model.linear3.bias.shape, 0.03)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=alpha)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(K):\n",
    "    tmp_losses = []\n",
    "    for batch in train_dataloader: \n",
    "        optimizer.zero_grad()\n",
    "        train_losses = 0\n",
    "\n",
    "        input_batch = batch[0]\n",
    "        label_batch = batch[1]\n",
    "\n",
    "        perm = np.random.permutation(np.arange(B)) \n",
    "        for i in range(B):\n",
    "            idx = perm[i%B]\n",
    "            input = input_batch[idx]\n",
    "            label = label_batch[idx]\n",
    "            output = model(input)\n",
    "            loss = criterion(output, label.unsqueeze(-1)) # Ensure dimensions are the same\n",
    "            train_losses += loss\n",
    "\n",
    "        train_losses = train_losses/B\n",
    "        tmp_losses.append(train_losses)\n",
    "        # losses.append(train_losses)\n",
    "        train_losses.requires_grad_(True)\n",
    "        train_losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(np.mean([x.detach().numpy()  for x in tmp_losses]))\n",
    "    print(f\"epoch : {epoch}, train_loss : {torch.mean(torch.FloatTensor(train_losses))}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "    # plt.plot(range(10,N//B*K), losses[10:], label = \"Train\")\n",
    "    plt.plot(range(10,K), losses[10:], label = \"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "losses = []\n",
    "model.eval\n",
    "test_losses = 0\n",
    "N = X_val.shape[0]//5\n",
    "for epoch in range(K):\n",
    "    tmp_losses = []\n",
    "    for i in range(N):\n",
    "        input = X_val[i]\n",
    "        label = y_val[i]\n",
    "        output = model(input)\n",
    "        loss = criterion(output,label.unsqueeze(-1))\n",
    "        test_losses += loss\n",
    "    losses.append(test_losses/N)\n",
    "    print(f\"epoch : {epoch}, test_loss : {torch.mean(torch.FloatTensor(test_losses))}\")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # plt.plot(range(K), losses, label = \"Test\")\n",
    "    plt.plot(range(K), losses, label = \"Test\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xx = torch.linspace(-2,2,1024).unsqueeze(1)\n",
    "    plt.plot(X_train,y_train,'rx',label='Data points')\n",
    "    plt.plot(xx,f_true(xx),'r',label='True Fn')\n",
    "    plt.plot(xx, model(xx),label='Learned Fn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "When plotting torch tensors, you want to work with the\n",
    "torch.no_grad() context manager.\n",
    "\n",
    "When you call plt.plot(...) the torch tensors are first converted into\n",
    "numpy arrays and then the plotting proceeds.\n",
    "However, our trainable model has requires_grad=True to allow automatic\n",
    "gradient computation via backprop, and this option prevents \n",
    "converting the torch tensor output by the model to a numpy array.\n",
    "Using the torch.no_grad() context manager resolves this problem\n",
    "as all tensors are set to requires_grad=False within the context manager.\n",
    "\n",
    "An alternative to using the context manager is to do \n",
    "plt.plot(xx, model(xx).detach().clone())\n",
    "The .detach().clone() operation create a copied pytorch tensor that\n",
    "has requires_grad=False.\n",
    "\n",
    "To be more precise, .detach() creates another tensor with requires_grad=False\n",
    "(it is detached from the computation graph) but this tensor shares the same\n",
    "underlying data with the original tensor. Therefore, this is not a genuine\n",
    "copy (not a deep copy) and modifying the detached tensor will affect the \n",
    "original tensor is weird ways. The .clone() further proceeds to create a\n",
    "genuine copy of the detached tensor, and one can freely manipulate and change it.\n",
    "(For the purposes of plotting, it is fine to just call .detach() without\n",
    ".clone() since plotting does not change the tensor.)\n",
    "\n",
    "This discussion will likely not make sense to most students at this point of the course.\n",
    "We will revisit this issue after we cover backpropagation.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6302)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "alpha = 0.1\n",
    "K = 1000\n",
    "B = 128\n",
    "N = 512\n",
    "\n",
    "def f_true(x) :\n",
    "    return (x-2) * np.cos(x*4)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X_train = torch.normal(0.0, 1.0, (N,))\n",
    "y_train = f_true(X_train)\n",
    "X_val = torch.normal(0.0, 1.0, (N//5,))\n",
    "y_val = f_true(X_val)\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=B)\n",
    "test_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
