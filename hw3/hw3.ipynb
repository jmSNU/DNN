{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem1 : Implementation\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "alpha = 0.1\n",
    "K = 1000\n",
    "B = 128\n",
    "N = 512\n",
    "\n",
    "def f_true(x) :\n",
    "    return (x-2) * np.cos(x*4)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X_train = torch.normal(0.0, 1.0, (N,))\n",
    "y_train = f_true(X_train)\n",
    "X_val = torch.normal(0.0, 1.0, (N//5,))\n",
    "y_val = f_true(X_val)\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=B)\n",
    "test_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=B)\n",
    "\n",
    "'''\n",
    "unsqueeze(1) reshapes the data into dimension [N,1],\n",
    "where is 1 the dimension of an data point.\n",
    "\n",
    "The batchsize of the test dataloader should not affect the test result\n",
    "so setting batch_size=N may simplify your code.\n",
    "In practice, however, the batchsize for the training dataloader\n",
    "is usually chosen to be as large as possible while not exceeding\n",
    "the memory size of the GPU. In such cases, it is not possible to\n",
    "use a larger batchsize for the test dataloader.\n",
    "'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # CIFAR-10 data is 32*32 images with 3 RGB channels\n",
    "    def __init__(self, input_dim=512) :\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64, bias=True)\n",
    "        self.linear2 = nn.Linear(64, 64, bias=True)\n",
    "        self.linear3 = nn.Linear(64, 1 , bias=True)\n",
    "        \n",
    "    ''' forward given input x '''\n",
    "    def forward(self, x) :\n",
    "        x = nn.functional.sigmoid(self.linear1(x))\n",
    "        x = nn.functional.sigmoid(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "model = MLP()\n",
    "\n",
    "model.linear1.weight.data = torch.normal (0 , 1 , model.lienar1.weight.shape)\n",
    "model.linear1.bias.data = torch.full(model.linear1.bias.shape, 0.03)\n",
    "\n",
    "model.linear2.weight.data = torch.normal (0 , 1 , model.lienar2.weight.shape)\n",
    "model.linear2.bias.data = torch.full(model.linear2.bias.shape, 0.03)\n",
    "\n",
    "model.linear3.weight.data = torch.normal (0 , 1 , model.lienar3.weight.shape)\n",
    "model.linear3.bias.data = torch.full(model.linear3.bias.shape, 0.03)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xx = torch.linspace(-2,2,1024).unsqueeze(1)\n",
    "    plt.plot(X_train,y_train,'rx',label='Data points')\n",
    "    plt.plot(xx,f_true(xx),'r',label='True Fn')\n",
    "    plt.plot(xx, model(xx),label='Learned Fn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "When plotting torch tensors, you want to work with the\n",
    "torch.no_grad() context manager.\n",
    "\n",
    "When you call plt.plot(...) the torch tensors are first converted into\n",
    "numpy arrays and then the plotting proceeds.\n",
    "However, our trainable model has requires_grad=True to allow automatic\n",
    "gradient computation via backprop, and this option prevents \n",
    "converting the torch tensor output by the model to a numpy array.\n",
    "Using the torch.no_grad() context manager resolves this problem\n",
    "as all tensors are set to requires_grad=False within the context manager.\n",
    "\n",
    "An alternative to using the context manager is to do \n",
    "plt.plot(xx, model(xx).detach().clone())\n",
    "The .detach().clone() operation create a copied pytorch tensor that\n",
    "has requires_grad=False.\n",
    "\n",
    "To be more precise, .detach() creates another tensor with requires_grad=False\n",
    "(it is detached from the computation graph) but this tensor shares the same\n",
    "underlying data with the original tensor. Therefore, this is not a genuine\n",
    "copy (not a deep copy) and modifying the detached tensor will affect the \n",
    "original tensor is weird ways. The .clone() further proceeds to create a\n",
    "genuine copy of the detached tensor, and one can freely manipulate and change it.\n",
    "(For the purposes of plotting, it is fine to just call .detach() without\n",
    ".clone() since plotting does not change the tensor.)\n",
    "\n",
    "This discussion will likely not make sense to most students at this point of the course.\n",
    "We will revisit this issue after we cover backpropagation.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
