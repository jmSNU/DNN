{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5 : Implementation\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL-Divergence\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 7.9708, Accuracy: 1873/1991 (94.07%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "'''\n",
    "Step 1: Prepare dataset\n",
    "'''\n",
    "# Use data with only 4 and 9 as labels: which is hardest to classify\n",
    "label_1, label_2 = 4, 9\n",
    "\n",
    "# MNIST training data\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (train_set.targets == label_1) + (train_set.targets == label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets == label_1] = -1\n",
    "train_set.targets[train_set.targets == label_2] = 1\n",
    "\n",
    "# MNIST testing data\n",
    "test_set = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets == label_1] = -1\n",
    "test_set.targets[test_set.targets == label_2] = 1\n",
    "\n",
    "'''\n",
    "Step 2: Define the neural network class\n",
    "'''\n",
    "class LR(nn.Module) :\n",
    "    '''\n",
    "    Initialize model\n",
    "        input_dim : dimension of given input data\n",
    "    '''\n",
    "    # MNIST data is 28x28 images\n",
    "    def __init__(self, input_dim=28*28) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    ''' forward given input x '''\n",
    "    def forward(self, x) :\n",
    "        return self.linear(x.float().view(-1, 28*28))\n",
    "\n",
    "'''\n",
    "Step 3: Create the model, specify loss function and optimizer.\n",
    "'''\n",
    "model = LR()                                   # Define a Neural Network Model\n",
    "\n",
    "def logistic_loss(output, target):\n",
    "    return -torch.nn.functional.logsigmoid(target*output)\n",
    "\n",
    "loss_function = logistic_loss                                                   # Specify loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)   # specify SGD with learning rate\n",
    "\n",
    "'''\n",
    "Step 4: Train model with SGD\n",
    "'''\n",
    "losses = []\n",
    "for _ in range(1000) :\n",
    "    # Sample a random data for training\n",
    "    ind = randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    # Clear previously computed gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # then compute gradient with forward and backward passes\n",
    "    train_loss = loss_function(model(image), label.float())\n",
    "    train_loss.backward()\n",
    "    losses.append(train_loss.item())\n",
    "    #(This syntax will make more sense once we learn about minibatches)\n",
    "\n",
    "    # perform SGD step (parameter update)\n",
    "    optimizer.step()\n",
    "'''\n",
    "Step 5: Test model (Evaluate the accuracy)\n",
    "'''\n",
    "test_loss, correct = 0, 0\n",
    "misclassified_ind = []\n",
    "correct_ind = []\n",
    "\n",
    "# Evaluate accuracy using test data\n",
    "for ind in range(len(test_set.data)) :\n",
    "\n",
    "    image, label = test_set.data[ind], test_set.targets[ind]\n",
    "\n",
    "    # evaluate model\n",
    "    output = model(image)\n",
    "\n",
    "    # Calculate cumulative loss\n",
    "    test_loss += loss_function(output, label.float()).item()\n",
    "\n",
    "    # Make a prediction\n",
    "    if output.item() * label.item() >= 0 :\n",
    "        correct += 1\n",
    "        correct_ind += [ind]\n",
    "    else:\n",
    "        misclassified_ind += [ind]\n",
    "\n",
    "# Print out the results\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /len(test_set.data), correct, len(test_set.data),\n",
    "        100. * correct / len(test_set.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 7.2848, Accuracy: 1861/1991 (93.47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "'''\n",
    "Step 1: Prepare dataset\n",
    "'''\n",
    "# Use data with only 4 and 9 as labels: which is hardest to classify\n",
    "label_1, label_2 = 4, 9\n",
    "\n",
    "# MNIST training data\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (train_set.targets == label_1) + (train_set.targets == label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets == label_1] = -1\n",
    "train_set.targets[train_set.targets == label_2] = 1\n",
    "\n",
    "# MNIST testing data\n",
    "test_set = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets == label_1] = -1\n",
    "test_set.targets[test_set.targets == label_2] = 1\n",
    "\n",
    "'''\n",
    "Step 2: Define the neural network class\n",
    "'''\n",
    "class LR(nn.Module) :\n",
    "    '''\n",
    "    Initialize model\n",
    "        input_dim : dimension of given input data\n",
    "    '''\n",
    "    # MNIST data is 28x28 images\n",
    "    def __init__(self, input_dim=28*28) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    ''' forward given input x '''\n",
    "    def forward(self, x) :\n",
    "        return self.linear(x.float().view(-1, 28*28))\n",
    "\n",
    "'''\n",
    "Step 3: Create the model, specify loss function and optimizer.\n",
    "'''\n",
    "model = LR()                                   # Define a Neural Network Model\n",
    "\n",
    "def loss(target,output):\n",
    "    return 0.5*(1-output)*((1-torch.sigmoid(-target))**2 + torch.sigmoid(target))+0.5*(1+output)*((torch.sigmoid(-target))**2+(1-torch.sigmoid(target))**2)\n",
    "\n",
    "loss_function = logistic_loss                                                   # Specify loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)   # specify SGD with learning rate\n",
    "\n",
    "'''\n",
    "Step 4: Train model with SGD\n",
    "'''\n",
    "losses = []\n",
    "for _ in range(1000) :\n",
    "    # Sample a random data for training\n",
    "    ind = randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    # Clear previously computed gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # then compute gradient with forward and backward passes\n",
    "    train_loss = loss_function(model(image), label.float())\n",
    "    train_loss.backward()\n",
    "    losses.append(train_loss.item())\n",
    "    #(This syntax will make more sense once we learn about minibatches)\n",
    "\n",
    "    # perform SGD step (parameter update)\n",
    "    optimizer.step()\n",
    "'''\n",
    "Step 5: Test model (Evaluate the accuracy)\n",
    "'''\n",
    "test_loss, correct = 0, 0\n",
    "misclassified_ind = []\n",
    "correct_ind = []\n",
    "\n",
    "# Evaluate accuracy using test data\n",
    "for ind in range(len(test_set.data)) :\n",
    "\n",
    "    image, label = test_set.data[ind], test_set.targets[ind]\n",
    "\n",
    "    # evaluate model\n",
    "    output = model(image)\n",
    "\n",
    "    # Calculate cumulative loss\n",
    "    test_loss += loss_function(output, label.float()).item()\n",
    "\n",
    "    # Make a prediction\n",
    "    if output.item() * label.item() >= 0 :\n",
    "        correct += 1\n",
    "        correct_ind += [ind]\n",
    "    else:\n",
    "        misclassified_ind += [ind]\n",
    "\n",
    "# Print out the results\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /len(test_set.data), correct, len(test_set.data),\n",
    "        100. * correct / len(test_set.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 : Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "'''\n",
    "Step 2: LeNet5\n",
    "'''\n",
    "\n",
    "# Modern LeNet uses this layer for C3\n",
    "class C3_layer_full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer_full, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n",
    "\n",
    "# Original LeNet uses this layer for C3\n",
    "class C3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer, self).__init__()\n",
    "        self.ch_in_3 = [[0, 1, 2],\n",
    "                        [1, 2, 3],\n",
    "                        [2, 3, 4],\n",
    "                        [3, 4, 5],\n",
    "                        [0, 4, 5],\n",
    "                        [0, 1, 5]] # filter with 3 subset of input channels\n",
    "        self.ch_in_4 = [[0, 1, 2, 3],\n",
    "                        [1, 2, 3, 4],\n",
    "                        [2, 3, 4, 5],\n",
    "                        [0, 3, 4, 5],\n",
    "                        [0, 1, 4, 5],\n",
    "                        [0, 1, 2, 5],\n",
    "                        [0, 1, 3, 4],\n",
    "                        [1, 2, 4, 5],\n",
    "                        [0, 2, 3, 5]] # filter with 4 subset of input channels\n",
    "        # put implementation here\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # put implementation here\n",
    "        pass\n",
    "    \n",
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                #C3_layer_full(),\n",
    "                C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = LeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# print total number of trainable parameters\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
